# SOS LASSO
# =========
regularization: soslasso

# Parameters
# ----------
# bias toggles whether or not to use a bias unit when fitting models
# alpha scales the SOS Lasso penalty associated with picking items from
# different groups.
#
# lambda scales the overall sparsity penalty (irrespective of groups).
#
# shape defines the group shape. Can be sphere or cube.
#
# diameter defines the size of the group in millimeters. If you specify a
# single value, it applies to all three dimensions. Otherwise, you can provide
# a list of values, one for each dimension x, y, z.
#
# overlap defines that amount of overlap between groups in millimeters. If you
# specify a single value, it applies to all three dimensions. Otherwise, you
# can provide a list of values, one for each dimension x, y, z.
#
# normalize allows you to specify a normalization method to apply to your data
# before fitting models. zscore is recommended (subtract mean and divide by
# standard deviation), but stdev will simply divide by the standard deviation
# without recentering, and 2norm will subtract the mean and divide by the
# 2-norm (which is the euclidean distance between each voxel and the origin).
bias: 0
alpha: [0.1,0.1,0.1,0.5,0.1,0.1,0.2,0.1,0.2,0.7]
lambda: [1.30,1.16,1.16,0.65,1.13,1.30,0.99,1.27,0.94,0.65]
shape: sphere
diameter: 18
overlap: 9
normalize: zscore

# Data and Metadata Paths
# =======================
# Paths to datasets that you want to fit models to. In the case of SOS Lasso,
# if multiple datasets are passed to a job they are all analyzed at the same
# time. Otherwise, if multiple datasets are passed to a job they are looped
# over, fitting independent models to each subject.
#
# data_var tells the program which variable to read out of each .mat file. Each
# job expects one data_var. You must always set data_var, but it is useful if
# you save multiple matrices within each .mat file. For example, if you
# pre-processed your data in two different ways, you might choose to store each
# dataset as a variable within as single file for the subject, rather than
# saving each to a separate file.
#
# Metadata is the path to your metadata file. The metadata structured array is
# described in the WholeBrain_RSA/demo/demo.m. Each job expects a single
# metadata file.
#
# metadata_var is the same as data_var.
data:
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp01.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp02.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp03.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp04.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp05.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp06.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp07.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp08.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp09.mat
  - /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/jlp10.mat
data_var: X
metadata: /Users/Chris/src/WholeBrain_MVPA/demo/demo01/data/metadata_TR5_new.mat
metadata_var: metadata

# Metadata Field References
# =========================
# K-fold Cross Validation
# -----------------------
# Cross validation (cv) indexes need to be specified in advance, and stored in
# the metadata structure for each subject under the field 'cvind'. 'cvind' must
# be a item-by-scheme matrix, where each value is a number from 1 to n, where n
# is the highest cv index. A scheme is simply a unique assignement of items to
# cv indexes, in case you want to try different schemes.
#
# Each job expects exactly 1 scheme.
#
# If a list of cvholdout indexes are provided to a single job, then they are
# looped over within the job and produce a separate model for each index
# trained and tested on the appropriate subsets.
#
# Each job expects exactly 1 "finalholdout" index. The final holdout index is
# used during the tuning phase when fitting models are many different parameter
# values. This chunk of the data is completely dropped---it is neither trained
# nor tested on. These chunks are held out so that when you fit "final" models
# with the optimal parameters, there are parts of the data that had nothing to
# do with the parameter selection.
cvscheme: 1
cvholdout: [1,2,3,4,5,6,7,8,9,10]
finalholdout: 0

# Targets
# -------
# These fields check against metadata.targets.label and metadata.targets.type,
# respectively, to select the right target. See WholeBrain_MVPA/demo/demo.m for
# how to define targets in the metadata structure.
target: faces
#target_type: category

# Coordinates
# -----------
# orientation is a way of indicating which set of coordinates should be
# referenced during the analysis. For SOS Lasso the choice of coordinates has
# affects how voxels are grouped. In all cases, this effects which coordinates
# are written out with your results. For SOS Lasso with multiple subjects you
# should use a common space orientation, since voxels are groups both within
# and across subjects. The value provided here is checked against
# metadata.coords.orientation to select the desired coordinates.
orientation: tlrc

# Filters
# -------
# filters are ways to subset your data. All filters must be predefined in
# metadata.filters. The values listed here are checked against
# metadata.filters.label. Two filters that apply to the same dimension will be
# combined with AND logic. If you provide a sublist of filters, they are
# combined using OR logic before being combined with the other filters of the
# same dimension.
filters:
  - rowfilter
  - colfilter

# WholeBrain_MVPA Options
# =======================
# SmallFootprint means "do not save model weights or predicted values". This
# might be useful when you are tuning over many, many parameters and you worry
# about running out of disk space.
#
# SaveResultsAs can be set to either mat or json. If json, the results
# structure is serialized and written to json-formatted text.
#
# subject_id_fmt tells the program how to determine the subject id from your
# data file naming convention. Internally, the program will extract the subject
# id from the filename using sscanf which is a MATLAB builtin. So experiment
# with sscanf to come up with the right format string for your needs, and then
# put that format string here.
#
# If you are not running on HTCondor, you can drop the executable and wrapper
# lines.
SmallFootprint: 0
SaveResultsAs: mat
subject_id_fmt: jlp%d.mat
executable: "/Users/Chris/src/WholeBrain_MVPA/bin/WholeBrain_MVPA"
wrapper: "/Users/Chris/src/WholeBrain_MVPA/run_WholeBrain_MVPA.sh"

# condortools/setupJob Options
# ============================
# EXPAND: Accepts fields that contain a list. Each element in the list will be
# assigned to a separate job. If multiple fields are provided to EXPAND, their
# lists are crossed. Fields nested in sublists under EXPAND are linked.
#
# Example 1:
# a: [1,2]
# b: [1,2]
# EXPAND: [a,b]
# ==> 4 jobs, a=1,b=1; a=1,b=2; a=2,b=1; a=2,b=2.
#
# Example 2:
# a: [1,2]
# b: [1,2]
# EXPAND: [[a,b]]
# ==> 2 jobs, a=1,b=1; a=2,b=2.
#
# Example 3:
# a: [1,2]
# b: [1,2]
# EXPAND: []
# ==> 1 jobs, a=[1,2],b=[1,2].
#
# COPY and URLS: Both expect fields that contain a file path or list of file
# paths. Files are either copied into the job file structure or written into a
# URLS file (which are referenced on the execute-node of a distributed job to
# retrieve files from a proxy server). These operations happen after EXPAND
# takes effect, so lists of files can be distributed to specific jobs.
EXPAND:
  - [cvholdout,alpha,lambda]
# If you are not running on HTCondor, you can (probably) replace the following with:
COPY: []
URLS: []
