
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Running SOS Lasso with WholeBrain_MVPA</title><meta name="generator" content="MATLAB 8.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-08-13"><meta name="DC.source" content="demo00.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Running SOS Lasso with <b>WholeBrain_MVPA</b></h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">The data</a></li><li><a href="#2">Targets metadata.targets</a></li><li><a href="#3">Cross-validation</a></li><li><a href="#4">Filters</a></li><li><a href="#5">Coordinates</a></li><li><a href="#6">Putting it all together</a></li><li><a href="#7">Save the data to disk</a></li><li><a href="#8">Conclusion</a></li><li><a href="#9">Define a parameter file</a></li><li><a href="#10">Run <b>WholeBrain_MVPA</b>: SOS Lasso</a></li><li><a href="#11">Run <b>WholeBrain_MVPA</b>: Searchlight</a></li></ul></div><h2>The data<a name="1"></a></h2><p>The fMRI data should be formatted so that each row is a training <i>example</i>, and the columns are the <i>features</i> that express each example. An <i>example</i> might correspond to the activation over voxels at a particular point in time, the beta values or t-values resulting from an initial univariate model of a design matrix convolved with an HRF, or anything else you like. Each feature, for our purposes, is a voxel.</p><div><ul><li>This matrix can be named whatever you like.</li><li>It must be saved in a .mat file, either on it's own or along with other   variables.</li><li>The .mat file itself can be named whatever you like, with the one   constraint that <b>WholeBrain_MVPA</b> will expect a subject number to be   present somewhere in the filename.</li><li>Numbers can be zero padded.</li></ul></div><p>In this demo, we'll call the matrix <tt>X</tt>, which will be the only variable we save to the .mat file.</p><p>Imagine a study with 100 unique items, sampled equally from two categories or belonging to two experimental conditions. Further, imagine that there are 10,000 voxels in the cortex of this subject.</p><p>Let <tt>y</tt> represent the category or condition labels of the items. This is the target structure that we will be modelling based on the data in <tt>X</tt>. Since all methods in this package are binary classifiers, <tt>y</tt> should be binary. We'll make <tt>y</tt> dependent on independent contributions from 10 voxels.</p><pre class="codeinput">nitems = 100;
nvoxels = 1000;
X = randn(nitems, nvoxels);

b = zeros(nvoxels, 1);
b(1:10) = 10;
y = X*b;
y = y &gt; median(y);

tabulate(y);
</pre><pre class="codeoutput">  Value    Count   Percent
      0       50     50.00%
      1       50     50.00%
</pre><h2>Targets metadata.targets<a name="2"></a></h2><p>Information about targets (i.e., possible y vectors) should be stored in a structure with 5 required fields:</p><div><ol><li><tt>label</tt></li><li><tt>type</tt></li><li><tt>targets</tt></li><li><tt>sim_source</tt></li><li><tt>sim_metric</tt></li></ol></div><p>Only the first three are relevant for classification analyses, but all must be present.</p><pre class="codeinput">TARGETS(1) = struct(<span class="keyword">...</span>
  <span class="string">'label'</span>,<span class="string">'faces'</span>,<span class="keyword">...</span>
  <span class="string">'type'</span>,<span class="string">'category'</span>,<span class="keyword">...</span>
  <span class="string">'target'</span>,y, <span class="keyword">...</span>
  <span class="string">'sim_source'</span>,[],<span class="string">'sim_metric'</span>,[]);
TARGETS(2) = struct(<span class="keyword">...</span>
  <span class="string">'label'</span>,<span class="string">'places'</span>,<span class="keyword">...</span>
  <span class="string">'type'</span>,<span class="string">'category'</span>,<span class="keyword">...</span>
  <span class="string">'target'</span>,~y, <span class="keyword">...</span>
  <span class="string">'sim_source'</span>,[],<span class="string">'sim_metric'</span>,[]);
</pre><h2>Cross-validation<a name="3"></a></h2><p>Next, we'll define indexes for cross validation. A single cross-validation <i>scheme</i> is a vector, containing the whole-numbers <tt>1:k</tt>, where <tt>k</tt> is the number of cross validation folds.</p><p>The Matlab function <tt>cvpartition</tt>, which belongs to the Statistics Toolbox, is very helpful for generating cross-validation schemes. One of the major advantages of using <tt>cvpartition</tt> is that, if you give it a categorical target structure as its first argument, it will pick <tt>k</tt> holdout sets where each set has a balanced sample of each category.</p><p>For example, try:</p><pre class="language-matlab">y = repmat((1:3)', 20, 1);
c = cvpartition(y, <span class="string">'kfold'</span>, 10);
disp(c);
<span class="keyword">for</span> i = 1:10
    z = test(c, i);
    disp(<span class="string">'Holdout index:'</span>);
    disp(find(z)');
    tabulate(y(z));
    fprintf(<span class="string">'\n\n'</span>);
<span class="keyword">end</span>
</pre><p>You can pre-specify multiple cross-validation schemes. If you specify more than one, each individual scheme will be a column in a matrix. When defining your analysis, you will provide a <tt>cvscheme</tt>, which will simply be a column-index into this matrix you are defining.</p><p>For example, let's set up 10 cross validation schemes, each defining a different data partition for 10-fold cross validation.</p><pre class="codeinput">nschemes = 10;
nfolds = 10;
SCHEMES = zeros(nitems, nschemes);
<span class="keyword">for</span> iScheme = 1:nschemes
    c = cvpartition(y,<span class="string">'KFold'</span>, nfolds);
    <span class="keyword">for</span> iFold = 1:nfolds
        SCHEMES(:,iScheme) = SCHEMES(:,iScheme) + (test(c, iFold) * iFold);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2>Filters<a name="4"></a></h2><p>You may want to be able to select/exclude subsets of voxels and items without needed to make multiple copies of the data. By specifying filters, you can pre-specify these subsets and apply them programmatically.</p><p>A filter is represented as a structure with 3 required fields:</p><div><ol><li><tt>label</tt> - names the filter so that it can be easily referenced.</li><li><tt>dimension</tt> - encodes whether the filter applies to rows (1) or columns (2) of X.</li><li><tt>filter</tt> - a binary vectory that represents the filter itself.</li></ol></div><p>Here, lets set up (totally arbitrarily) two filters. The first will define a region of interest and the second will exclude outliers.</p><pre class="codeinput">z = [true(500,1);false(9500,1)];
FILTERS(1) = struct(<span class="string">'label'</span>,<span class="string">'ROI01'</span>, <span class="string">'dimension'</span>, 2, <span class="string">'filter'</span>, z);
z = [true(98,1);false(2,1)];
FILTERS(2) = struct(<span class="string">'label'</span>,<span class="string">'GoodRows'</span>, <span class="string">'dimension'</span>, 1, <span class="string">'filter'</span>, z);
</pre><h2>Coordinates<a name="5"></a></h2><p>Up to this point, we've considered our data strictly as a matrix, with rows as examples and columns as features. Of course, voxels exist in space, and in order to ultimately map weight vectors or information maps back into a brain-space for visualization, you'll need to know the coordinate of each voxel.</p><p>Like filters, coordinates are represented in a structure that allows descriptive labeling, and also allows you to store the three index/coordinate types that AFNI will output (via <b>3dMaskDump</b>):</p><div><ul><li><tt>ind</tt> - 1-dimensional index.</li><li><tt>ijk</tt> - the integer-valued data-space coordinates.</li><li><tt>xyz</tt> - the "real world" decimal valued coordinates (in <i>mm</i>).</li></ul></div><p>Something a bit awkward and idiosyncratic about the coordinate structure is that the field that functions the same as <tt>label</tt> so many other places in the metadata structure is, here, called <tt>orientation</tt>. This is because I figured the only reason one would have multiple coordinate spaces per subject is to represent the result of multiple warps. Each coordinate space resulting from a warp, in AFNI parlance, is an orientation. So, that is the etymology of the <tt>orientation</tt> label.</p><p>That all being said, I've been meaning to rename <tt>orientation</tt> to <tt>label</tt> for ages for the sake of internal consistency, and because <b>WholeBrain_MVPA</b> does not do anything special behind the scenes with the field. It is treated exactly as values in the <tt>label</tt> field are treated elsewhere. Which means there is nothing special about using <i>orig</i> or <i>tlrc</i> as values for the orientation field. (If there is part of the code the breaks if you don't use <i>orig</i> or <i>tlrc</i>, that would qualify as a bug!)</p><pre class="codeinput">ind = (1:nvoxels)';
ijk = [(1:nvoxels)',ones(nvoxels,1),ones(nvoxels,1)];
xyz = bsxfun(@minus, ijk, [nvoxels/2, 1, 1]);

COORDS(1) = struct(<span class="string">'orientation'</span>,<span class="string">'orig'</span>,<span class="string">'ind'</span>,ind,<span class="string">'ijk'</span>,ijk,<span class="string">'xyz'</span>,xyz);
COORDS(2) = struct(<span class="string">'orientation'</span>,<span class="string">'tlrc'</span>,<span class="string">'ind'</span>,ind,<span class="string">'ijk'</span>,ijk,<span class="string">'xyz'</span>,xyz);
</pre><h2>Putting it all together<a name="6"></a></h2><p>The metadata object compiles these three items, along with a couple other bits of information, into a single structure. The metadata structure has several required fields:</p><div><ul><li><tt>subject</tt> - A numeric* subject ID.</li><li><tt>targets</tt> - Which will contain something like the <tt>TARGETS</tt> structure defined above.</li><li><tt>filters</tt> - Which will contain something like the <tt>FILTERS</tt> structure defined above.</li><li><tt>coords</tt> - Which will contain something like the <tt>COORDS</tt> structure defined above.</li><li><tt>cvind</tt> - Which will contain something like the <tt>SCHEMES</tt> matrix defined above.</li><li><tt>nrow</tt> - The number of rows in the data matrix for this subject (before applying any of the filters contained in <tt>metadata(s).filters</tt>).</li><li><tt>ncol</tt> - The number of columns in the data matrix for this subject (before applying any of the filters contained in <tt>metadata(s).filters</tt>).</li></ul></div><p>There will be a metadata structure for each subject, compiled into a structured array. Although in the example below subjects 100 and 101 are the same aside from their subject numbers, in practice they could be given different information.</p><pre class="codeinput"><span class="comment">% Subject 100</span>
metadata(1).subject = 100;
metadata(1).targets = TARGETS;
metadata(1).filters = FILTERS;
metadata(1).coords = COORDS;
metadata(1).cvind = SCHEMES;
metadata(1).nrow = nitems;
metadata(1).ncol = nvoxels;

<span class="comment">% Subject 101</span>
metadata(2).subject = 101;
metadata(2).targets = TARGETS;
metadata(2).filters = FILTERS;
metadata(2).coords = COORDS;
metadata(2).cvind = SCHEMES;
metadata(2).nrow = nitems;
metadata(2).ncol = nvoxels;
</pre><h2>Save the data to disk<a name="7"></a></h2><p>Despite having data and metadata organized properly in memory, before working with <b>WholeBrain_MVPA</b> we need to write the data to disk. The reason for this is that <b>WholeBrain_MVPA</b> is not written to be used interactively, but rather to facilitate to use in headless, batch applications particularly on distributed computing systems. <b>WholeBrain_MVPA</b> accepts paths to files on disk, as well as many other parameters. The data and metadata should be saved to a central location where it can be easily referenced. These files can be named whatever you like. You will be referencing them with explicit paths, and <b>WholeBrain_MVPA</b> does not make any assumptions about them. The program does assume that the <b>variable</b> names are X and metadata, but this default can be overwritten with certain parameters to <b>WholeBrain_MVPA</b> (data_var and metadata_var) if you prefer another convention.</p><pre class="codeinput">subjects = [metadata.subject];
datadir = <span class="string">'./shared'</span>;
<span class="keyword">if</span> ~exist(datadir,<span class="string">'dir'</span>)
    mkdir(datadir);
<span class="keyword">end</span>
<span class="keyword">for</span> iSubj = 1:2
  s = subjects(iSubj);
  X = randn(nitems, nvoxels);
  X(1:50,1:20) = X(1:50,1:20) + 2;
  filename = sprintf(<span class="string">'s%03d.mat'</span>, s);
  filepath = fullfile(datadir,filename);
  save(filepath, <span class="string">'X'</span>);
<span class="keyword">end</span>
save(fullfile(datadir,<span class="string">'metadata.mat'</span>), <span class="string">'metadata'</span>);
</pre><h2>Conclusion<a name="8"></a></h2><p>Setting up the metadata structure can be a bit of a hassle, and is by its nature labor intensive. It is important to take great care when setting it up, because the metadata structure is the primary data structure that <b>WholeBrain_MVPA</b> will reference when attempting to run analyses.</p><p>The good news is, that you do not need to set it up anew for every analysis. On the contrary, in an ideal world you should only need to specify the metadata structure one time per project, unless new filters, targets, or coordinates become necessary, or the underlying data matrices themselves change in some way.</p><p>Once the metadata structure is defined and saved to the hard-drive, we can get on with the more interesting work of specifying anayses.</p><h2>Define a parameter file<a name="9"></a></h2><p><b>WholeBrain_MVPA</b>, despite being written as a Matlab function, is a pretty atypical function.</p><p>First of all, it does not return anything. All results are written to disk. In addition, while it is possible to invoke <b>WholeBrain_MVPA</b> from within a script or at the interactive terminal, it is designed to take instructions from a json-formatted parameter filelook for a parameter file if no arguments are provided. This all makes <b>WholeBrain_MVPA</b> a bit counter-intuitive.</p><p>However, these design choices make much more sense when considered in a distributed computing environment. <b>WholeBrain_MVPA</b> can be deployed to a system, along with a json file containing parameters, and it will parse the file and execute according to the instructions. It is designed to be executed with bare minimum interaction.</p><p>Defining a parameter file is simple. See the documentation for a list of valid parameters. <b>WholeBrain_MVPA</b> reads json (<a href="http://www.json.org/">http://www.json.org/</a>), which is a widely used text-based syntax for representing structured data.</p><p><b>The file must be named params.json!</b></p><p>I call this out in bold because it is important... but in practice, it isn't something you will need to think much about. Another bit of code, part of my <a href="https://github.com/crcox/condortools">CondorTools</a> repository, called <b>setupJobs</b>, will write you params.json files for you. But we are not quite there yet.</p><p>To read and write json, you will need jsonlab (<a href="http://www.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files">http://www.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files</a>) which is bundled with <b>WholeBrain_MVPA</b>:</p><pre class="codeinput"><span class="keyword">if</span> ~exist(<span class="string">'savejson'</span>,<span class="string">'file'</span>)
    addpath(GetFullPath(fullfile(pwd,<span class="string">'..'</span>,<span class="string">'..'</span>,<span class="string">'dependencies'</span>,<span class="string">'jsonlab'</span>)));
<span class="keyword">end</span>

<span class="comment">% Put the parameter file where you want to run the analysis. Paths can be</span>
<span class="comment">% relative with respect to where you execute *WholeBrain_MVPA*, but in most cases</span>
<span class="comment">% it will probably make sense for them to be absolute. The following should</span>
<span class="comment">% translate into a valid json file for the purpose of this demo.</span>
params = struct(<span class="string">'regularization'</span>, <span class="string">'soslasso'</span>, <span class="string">'bias'</span>, false, <span class="string">'alpha'</span>, 0.4200556,<span class="keyword">...</span>
    <span class="string">'lambda'</span>, 0.5863, <span class="string">'shape'</span>, <span class="string">'sphere'</span>, <span class="string">'diameter'</span>, 18, <span class="string">'overlap'</span>, 9,<span class="keyword">...</span>
    <span class="string">'cvscheme'</span>, 1,<span class="string">'cvholdout'</span>, 1:10, <span class="string">'finalholdout'</span>, 0, <span class="string">'target'</span>, <span class="string">'faces'</span>,<span class="keyword">...</span>
    <span class="string">'data'</span>, {{<span class="string">'./shared/s100.mat'</span>, <span class="string">'./shared/s101.mat'</span>}}, <span class="string">'data_var'</span>, <span class="string">'X'</span>,<span class="keyword">...</span>
    <span class="string">'normalize'</span>, <span class="string">'zscore'</span>, <span class="string">'metadata'</span>, <span class="string">'./shared/metadata.mat'</span>,<span class="keyword">...</span>
    <span class="string">'metadata_var'</span>, <span class="string">'metadata'</span>, <span class="string">'orientation'</span>, <span class="string">'tlrc'</span>, <span class="string">'filters'</span>, <span class="keyword">...</span>
    {{<span class="string">'ROI01'</span>,<span class="string">'GoodRows'</span>}}, <span class="string">'SmallFootprint'</span>, false, <span class="string">'debug'</span>, false,<span class="keyword">...</span>
    <span class="string">'SaveResultsAs'</span>,<span class="string">'json'</span>,<span class="string">'subject_id_fmt'</span>,<span class="string">'s%d.mat'</span>);
savejson(<span class="string">''</span>,params,<span class="string">'FileName'</span>,<span class="string">'params.json'</span>,<span class="string">'ForceRootName'</span>,false);
</pre><h2>Run <b>WholeBrain_MVPA</b>: SOS Lasso<a name="10"></a></h2><pre>==============================
With data and metadata structured properly and saved to disk, and with a
parameter file named params.json in a folder where you would like to execute
the analysis and return results, all that remains is to boot up Matlab in the
directory that contains 'params.json' and execute _WholeBrain_MVPA()_ at the
command prompt. If you have compiled *WholeBrain_MVPA* into an executable (as
would be necessary on a distributed computing cluster), you can execute
*WholeBrain_MVPA* directly from the command line. In either case, it will read
the parameter file and begin analysis. When it completes you will find a
results.mat (or results.json) file in the directory where *WholeBrain_MVPA* was
executed.</pre><pre class="codeinput"><span class="keyword">if</span> ~exist(<span class="string">'WholeBrain_MVPA'</span>,<span class="string">'file'</span>)
    addpath(GetFullPath(fullfile(pwd,<span class="string">'..'</span>,<span class="string">'..'</span>,<span class="string">'src'</span>)));
<span class="keyword">end</span>
WholeBrain_MVPA()
</pre><pre class="codeoutput">
ans = 

                  AdlasOpts: [1x1 struct]
                      alpha: 0.4201
                       bias: 0
                       COPY: []
                  cvholdout: [1 2 3 4 5 6 7 8 9 10]
                   cvscheme: 1
                       data: {'./shared/s100.mat'  './shared/s101.mat'}
                   data_var: 'X'
                     debias: 0
                      debug: 0
                   diameter: 18
                environment: 'condor'
                 executable: []
                    filters: {'ROI01'  'GoodRows'}
               finalholdout: 0
                     lambda: 0.5863
                   metadata: './shared/metadata.mat'
               metadata_var: 'metadata'
                  normalize: 'zscore'
                orientation: 'tlrc'
                    overlap: 9
                   PARALLEL: 0
          PermutationMethod: 'simple'
            PermutationTest: 0
                 RandomSeed: 0
             regularization: 'soslasso'
    RestrictPermutationByCV: 0
            SanityCheckData: []
              SaveResultsAs: 'json'
                searchlight: 0
                      shape: 'sphere'
               slclassifier: 'gnb_searchmight'
             slpermutations: 0
                   slradius: []
                slTestToUse: 'accuracyOneSided_analytical'
             SmallFootprint: 0
             subject_id_fmt: 's%d.mat'
                     target: 'faces'
                target_type: []
                       URLS: []
                    wrapper: []

Loading X from  ./shared/s100.mat...
Loading X from  ./shared/s101.mat...

Loading similarity structure
----------------------------
target_label: faces
        type: category

Including Bias Unit       : [ NO]
Normalizing columns of X  : [zscore]
Final holdout index       : [  0]
Data loaded and processed.
PermutationTest: 0
        subj   alpha  lambda    test err   train err   test diff  train diff       n vox
cv   1:   1 |  0.42 |  0.59 |         4 |        44 |    0.0000 |    0.0000 |         0 |
cv   1:   2 |  0.42 |  0.59 |         4 |        44 |    0.0000 |    0.0000 |         0 |
cv   2:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   2:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   3:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   3:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   4:   1 |  0.42 |  0.59 |         4 |        44 |    0.0000 |    0.0000 |         0 |
cv   4:   2 |  0.42 |  0.59 |         4 |        44 |    0.0000 |    0.0000 |         0 |
cv   5:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   5:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   6:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   6:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   7:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   7:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   8:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   8:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   9:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv   9:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv  10:   1 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
cv  10:   2 |  0.42 |  0.59 |         5 |        43 |    0.0000 |    0.0000 |         0 |
logged 20 results in memory.
  Name         Size            Bytes  Class     Attributes

  results      1x20            78140  struct              

Saving 20 results
	results.mat
Done!
</pre><h2>Run <b>WholeBrain_MVPA</b>: Searchlight<a name="11"></a></h2><pre>================================
Put the parameter file where you want to run the analysis. Paths can be
relative with respect to where you execute WholeBrain_MVPA, but in most cases
it will probably make sense for them to be absolute. The following should
translate into a valid json file for the purpose of this demo.</pre><pre class="codeinput">params = struct(<span class="string">'algorithm'</span>, <span class="string">'soslasso'</span>, <span class="string">'bias'</span>, false, <span class="string">'alpha'</span>, 0.4200556,<span class="keyword">...</span>
    <span class="string">'lambda'</span>, 0.5863, <span class="string">'shape'</span>, <span class="string">'sphere'</span>, <span class="string">'diameter'</span>, 18, <span class="string">'overlap'</span>, 9,<span class="keyword">...</span>
    <span class="string">'cvscheme'</span>, 1,<span class="string">'cvholdout'</span>, 1:10, <span class="string">'finalholdout'</span>, 0, <span class="string">'target'</span>, <span class="string">'faces'</span>,<span class="keyword">...</span>
    <span class="string">'data'</span>, {{<span class="string">'./shared/s100.mat'</span>, <span class="string">'./shared/s101.mat'</span>}}, <span class="string">'data_var'</span>, <span class="string">'X'</span>,<span class="keyword">...</span>
    <span class="string">'normalize'</span>, <span class="string">'zscore'</span>, <span class="string">'metadata'</span>, <span class="string">'./shared/metadata.mat'</span>,<span class="keyword">...</span>
    <span class="string">'metadata_var'</span>, <span class="string">'metadata'</span>, <span class="string">'orientation'</span>, <span class="string">'tlrc'</span>, <span class="string">'filters'</span>, <span class="keyword">...</span>
    {{<span class="string">'ROI01'</span>,<span class="string">'GoodRows'</span>}}, <span class="string">'SmallFootprint'</span>, false, <span class="string">'debug'</span>, false,<span class="keyword">...</span>
    <span class="string">'SaveResultsAs'</span>,<span class="string">'json'</span>,<span class="string">'subject_id_fmt'</span>,<span class="string">'s%d.mat'</span>);
savejson(<span class="string">''</span>,params,<span class="string">'FileName'</span>,<span class="string">'params.json'</span>,<span class="string">'ForceRootName'</span>,false);

<span class="comment">% Compile Results</span>
<span class="comment">% ===============</span>
<span class="comment">% If you are using *WholeBrain_MVPA* on a distributed computing cluster, you will</span>
<span class="comment">% quickly find that the volume of results is difficult to manage effectively. I</span>
<span class="comment">% have written some utility functions in *WholeBrain_MVPA*/util that attempt to</span>
<span class="comment">% facilitate common actions, like loading data from many jobs into a single</span>
<span class="comment">% matlab structure, writing tables of data, dumping coordinates of identified</span>
<span class="comment">% voxels, etc.</span>
<span class="comment">% Alternatively, you may find that your volume of data demands a database</span>
<span class="comment">% solution. Although the default is to return data in .mat files, which makes</span>
<span class="comment">% it easy to read back into matlab, results can also be output in json format</span>
<span class="comment">% which facilitates storing in a SQL or NoSQL database like MongoDB. Setting up</span>
<span class="comment">% such a database solution is far beyond the scope of this demo, but the squall</span>
<span class="comment">% project (github.com/ikinsella/squall) is a developing solution that utilizes</span>
<span class="comment">% MongoDB to great effect.</span>
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Running SOS Lasso with *WholeBrain_MVPA*
% 

%% The data
% The fMRI data should be formatted so that each row is a training
% _example_, and the columns are the _features_ that express each example.
% An _example_ might correspond to the activation over voxels at a
% particular point in time, the beta values or t-values resulting from an
% initial univariate model of a design matrix convolved with an HRF, or
% anything else you like. Each feature, for our purposes, is a voxel. 
%
% * This matrix can be named whatever you like.
% * It must be saved in a .mat file, either on it's own or along with other
%   variables.
% * The .mat file itself can be named whatever you like, with the one
%   constraint that *WholeBrain_MVPA* will expect a subject number to be
%   present somewhere in the filename.
% * Numbers can be zero padded.
%
% In this demo, we'll call the matrix |X|, which will be the only variable we save to the .mat file.
%
% Imagine a study with 100 unique items, sampled equally from two
% categories or belonging to two experimental conditions. Further, imagine that
% there are 10,000 voxels in the cortex of this subject.
%
% Let |y| represent the category or condition labels of the items. This is
% the target structure that we will be modelling based on the data in |X|.
% Since all methods in this package are binary classifiers, |y| should be
% binary. We'll make |y| dependent on independent contributions from 10
% voxels.

nitems = 100;
nvoxels = 1000;
X = randn(nitems, nvoxels);

b = zeros(nvoxels, 1);
b(1:10) = 10;
y = X*b;
y = y > median(y);

tabulate(y);

%% Targets metadata.targets
% Information about targets (i.e., possible y vectors) should be stored in a
% structure with 5 required fields:
% 
% # |label|
% # |type|
% # |targets|
% # |sim_source|
% # |sim_metric|
%
% Only the first three are relevant for classification analyses, but all
% must be present.

TARGETS(1) = struct(...
  'label','faces',...
  'type','category',...
  'target',y, ...
  'sim_source',[],'sim_metric',[]);
TARGETS(2) = struct(...
  'label','places',...
  'type','category',...
  'target',~y, ...
  'sim_source',[],'sim_metric',[]);

%% Cross-validation
% Next, we'll define indexes for cross validation. A single
% cross-validation _scheme_ is a vector, containing the whole-numbers |1:k|,
% where |k| is the number of cross validation folds. 
% 
% The Matlab function |cvpartition|, which belongs to the Statistics Toolbox,
% is very helpful for generating cross-validation schemes. One of the major
% advantages of using |cvpartition| is that, if you give it a categorical
% target structure as its first argument, it will pick |k| holdout sets
% where each set has a balanced sample of each category.
%
% For example, try:
%
%   y = repmat((1:3)', 20, 1);
%   c = cvpartition(y, 'kfold', 10);
%   disp(c);
%   for i = 1:10
%       z = test(c, i);
%       disp('Holdout index:');
%       disp(find(z)');
%       tabulate(y(z));
%       fprintf('\n\n');
%   end
%
% You can pre-specify multiple cross-validation schemes. If you specify
% more than one, each individual scheme will be a column in a matrix. When
% defining your analysis, you will provide a |cvscheme|, which will simply
% be a column-index into this matrix you are defining.
%
% For example, let's set up 10 cross validation schemes, each defining a
% different data partition for 10-fold cross validation.
nschemes = 10;
nfolds = 10;
SCHEMES = zeros(nitems, nschemes);
for iScheme = 1:nschemes
    c = cvpartition(y,'KFold', nfolds);
    for iFold = 1:nfolds
        SCHEMES(:,iScheme) = SCHEMES(:,iScheme) + (test(c, iFold) * iFold);
    end
end

%% Filters
% You may want to be able to select/exclude subsets of voxels and items without
% needed to make multiple copies of the data. By specifying filters, you can
% pre-specify these subsets and apply them programmatically.
%
% A filter is represented as a structure with 3 required fields:
%
% # |label| - names the filter so that it can be easily referenced.
% # |dimension| - encodes whether the filter applies to rows (1) or
% columns (2) of X.
% # |filter| - a binary vectory that represents the filter
% itself.
%
% Here, lets set up (totally arbitrarily) two filters. The first will
% define a region of interest and the second will exclude outliers.
z = [true(500,1);false(9500,1)];
FILTERS(1) = struct('label','ROI01', 'dimension', 2, 'filter', z);
z = [true(98,1);false(2,1)];
FILTERS(2) = struct('label','GoodRows', 'dimension', 1, 'filter', z);

%% Coordinates
% Up to this point, we've considered our data strictly as a matrix, with
% rows as examples and columns as features. Of course, voxels exist in
% space, and in order to ultimately map weight vectors or information maps
% back into a brain-space for visualization, you'll need to know the
% coordinate of each voxel.
%
% Like filters, coordinates are represented in a structure that allows
% descriptive labeling, and also allows you to store the three
% index/coordinate types that AFNI will output (via *3dMaskDump*):
% 
% * |ind| - 1-dimensional index.
% * |ijk| - the integer-valued data-space coordinates.
% * |xyz| - the "real world" decimal valued coordinates (in _mm_).
% 
% Something a bit awkward and idiosyncratic about the coordinate structure
% is that the field that functions the same as |label| so many other places
% in the metadata structure is, here, called |orientation|. This is because
% I figured the only reason one would have multiple coordinate spaces
% per subject is to represent the result of multiple warps. Each coordinate
% space resulting from a warp, in AFNI parlance, is an orientation. So,
% that is the etymology of the |orientation| label.
%
% That all being said, I've been meaning to rename |orientation| to |label|
% for ages for the sake of internal consistency, and because
% *WholeBrain_MVPA* does not do anything special behind the scenes with the
% field. It is treated exactly as values in the |label| field are treated
% elsewhere. Which means there is nothing special about using _orig_ or
% _tlrc_ as values for the orientation field. (If there is part of the code
% the breaks if you don't use _orig_ or _tlrc_, that would qualify as a
% bug!)
% 

ind = (1:nvoxels)';
ijk = [(1:nvoxels)',ones(nvoxels,1),ones(nvoxels,1)];
xyz = bsxfun(@minus, ijk, [nvoxels/2, 1, 1]);

COORDS(1) = struct('orientation','orig','ind',ind,'ijk',ijk,'xyz',xyz);
COORDS(2) = struct('orientation','tlrc','ind',ind,'ijk',ijk,'xyz',xyz);

%% Putting it all together
% The metadata object compiles these three items, along with a couple other
% bits of information, into a single structure. The metadata structure has
% several required fields:
% 
% * |subject| - A numeric* subject ID.
% * |targets| - Which will contain something like the |TARGETS| structure
% defined above.
% * |filters| - Which will contain something like the |FILTERS| structure
% defined above.
% * |coords| - Which will contain something like the |COORDS| structure
% defined above.
% * |cvind| - Which will contain something like the |SCHEMES| matrix
% defined above.
% * |nrow| - The number of rows in the data matrix for this subject (before
% applying any of the filters contained in |metadata(s).filters|).
% * |ncol| - The number of columns in the data matrix for this subject (before
% applying any of the filters contained in |metadata(s).filters|).
% 
% There will be a metadata structure for each subject, compiled into a
% structured array. Although in the example below subjects 100 and 101 are
% the same aside from their subject numbers, in practice they could be
% given different information.

% Subject 100
metadata(1).subject = 100;
metadata(1).targets = TARGETS;
metadata(1).filters = FILTERS;
metadata(1).coords = COORDS;
metadata(1).cvind = SCHEMES;
metadata(1).nrow = nitems;
metadata(1).ncol = nvoxels;

% Subject 101
metadata(2).subject = 101;
metadata(2).targets = TARGETS;
metadata(2).filters = FILTERS;
metadata(2).coords = COORDS;
metadata(2).cvind = SCHEMES;
metadata(2).nrow = nitems;
metadata(2).ncol = nvoxels;

%% Save the data to disk
% Despite having data and metadata organized properly in memory, before working
% with *WholeBrain_MVPA* we need to write the data to disk. The reason for this
% is that *WholeBrain_MVPA* is not written to be used interactively, but rather
% to facilitate to use in headless, batch applications particularly on
% distributed computing systems. *WholeBrain_MVPA* accepts paths to files on
% disk, as well as many other parameters.
% The data and metadata should be saved to a central location where it can be
% easily referenced.
% These files can be named whatever you like. You will be referencing them with
% explicit paths, and *WholeBrain_MVPA* does not make any assumptions about them.
% The program does assume that the *variable* names are X and metadata, but
% this default can be overwritten with certain parameters to *WholeBrain_MVPA*
% (data_var and metadata_var) if you prefer another convention.
subjects = [metadata.subject];
datadir = './shared';
if ~exist(datadir,'dir')
    mkdir(datadir);
end
for iSubj = 1:2
  s = subjects(iSubj);
  X = randn(nitems, nvoxels);
  X(1:50,1:20) = X(1:50,1:20) + 2;
  filename = sprintf('s%03d.mat', s);
  filepath = fullfile(datadir,filename);
  save(filepath, 'X');
end
save(fullfile(datadir,'metadata.mat'), 'metadata');

%% Conclusion
% Setting up the metadata structure can be a bit of a hassle, and is by
% its nature labor intensive. It is important to take great care when
% setting it up, because the metadata structure is the primary
% data structure that *WholeBrain_MVPA* will reference when attempting to
% run analyses.
%
% The good news is, that you do not need to set it up anew for every
% analysis. On the contrary, in an ideal world you should only need to
% specify the metadata structure one time per project, unless new filters,
% targets, or coordinates become necessary, or the underlying data matrices
% themselves change in some way.
%
% Once the metadata structure is defined and saved to the hard-drive, we
% can get on with the more interesting work of specifying anayses.

%% Define a parameter file
% *WholeBrain_MVPA*, despite being written as a Matlab function, is a
% pretty atypical function.
%
% First of all, it does not return anything. All results are written
% to disk. In addition, while it is possible to invoke *WholeBrain_MVPA* from
% within a script or at the interactive terminal, it is designed to take instructions from a json-formatted parameter filelook for a
% parameter file if no arguments are provided. This all makes
% *WholeBrain_MVPA* a bit counter-intuitive.
% 
% However, these design choices make
% much more sense when considered in a distributed computing environment.
% *WholeBrain_MVPA* can be deployed to a system, along with a json file
% containing parameters, and it will parse the file and execute according to
% the instructions. It is designed to be executed with bare minimum interaction.
%
% Defining a parameter file is simple. See the documentation for a list of
% valid parameters. *WholeBrain_MVPA* reads json (<http://www.json.org/>), which is
% a widely used text-based syntax for representing structured data.
%
% *The file must be named params.json!*
%
% I call this out in bold because it is important... but in practice, it
% isn't something you will need to think much about. Another bit of code,
% part of my <https://github.com/crcox/condortools CondorTools> repository,
% called *setupJobs*, will write you params.json files for you. But we are
% not quite there yet.
%
% To read and write json, you will need jsonlab
% (<http://www.mathworks.com/matlabcentral/fileexchange/33381-jsonlabREPLACE_WITH_DASH_DASHa-toolbox-to-encode-decode-json-files>)
% which is bundled with *WholeBrain_MVPA*:
if ~exist('savejson','file')
    addpath(GetFullPath(fullfile(pwd,'..','..','dependencies','jsonlab')));
end

% Put the parameter file where you want to run the analysis. Paths can be
% relative with respect to where you execute *WholeBrain_MVPA*, but in most cases
% it will probably make sense for them to be absolute. The following should
% translate into a valid json file for the purpose of this demo. 
params = struct('regularization', 'soslasso', 'bias', false, 'alpha', 0.4200556,...
    'lambda', 0.5863, 'shape', 'sphere', 'diameter', 18, 'overlap', 9,...
    'cvscheme', 1,'cvholdout', 1:10, 'finalholdout', 0, 'target', 'faces',...
    'data', {{'./shared/s100.mat', './shared/s101.mat'}}, 'data_var', 'X',...
    'normalize', 'zscore', 'metadata', './shared/metadata.mat',...
    'metadata_var', 'metadata', 'orientation', 'tlrc', 'filters', ...
    {{'ROI01','GoodRows'}}, 'SmallFootprint', false, 'debug', false,...
    'SaveResultsAs','json','subject_id_fmt','s%d.mat');
savejson('',params,'FileName','params.json','ForceRootName',false);

%% Run *WholeBrain_MVPA*: SOS Lasso
%  ==============================
% With data and metadata structured properly and saved to disk, and with a
% parameter file named params.json in a folder where you would like to execute
% the analysis and return results, all that remains is to boot up Matlab in the
% directory that contains 'params.json' and execute _WholeBrain_MVPA()_ at the
% command prompt. If you have compiled *WholeBrain_MVPA* into an executable (as
% would be necessary on a distributed computing cluster), you can execute
% *WholeBrain_MVPA* directly from the command line. In either case, it will read
% the parameter file and begin analysis. When it completes you will find a
% results.mat (or results.json) file in the directory where *WholeBrain_MVPA* was
% executed.
if ~exist('WholeBrain_MVPA','file')
    addpath(GetFullPath(fullfile(pwd,'..','..','src')));
end
WholeBrain_MVPA()

%%  Run *WholeBrain_MVPA*: Searchlight
%  ================================
% Put the parameter file where you want to run the analysis. Paths can be
% relative with respect to where you execute WholeBrain_MVPA, but in most cases
% it will probably make sense for them to be absolute. The following should
% translate into a valid json file for the purpose of this demo. 
params = struct('algorithm', 'soslasso', 'bias', false, 'alpha', 0.4200556,...
    'lambda', 0.5863, 'shape', 'sphere', 'diameter', 18, 'overlap', 9,...
    'cvscheme', 1,'cvholdout', 1:10, 'finalholdout', 0, 'target', 'faces',...
    'data', {{'./shared/s100.mat', './shared/s101.mat'}}, 'data_var', 'X',...
    'normalize', 'zscore', 'metadata', './shared/metadata.mat',...
    'metadata_var', 'metadata', 'orientation', 'tlrc', 'filters', ...
    {{'ROI01','GoodRows'}}, 'SmallFootprint', false, 'debug', false,...
    'SaveResultsAs','json','subject_id_fmt','s%d.mat');
savejson('',params,'FileName','params.json','ForceRootName',false);

% Compile Results
% ===============
% If you are using *WholeBrain_MVPA* on a distributed computing cluster, you will
% quickly find that the volume of results is difficult to manage effectively. I
% have written some utility functions in *WholeBrain_MVPA*/util that attempt to
% facilitate common actions, like loading data from many jobs into a single
% matlab structure, writing tables of data, dumping coordinates of identified
% voxels, etc.
% Alternatively, you may find that your volume of data demands a database
% solution. Although the default is to return data in .mat files, which makes
% it easy to read back into matlab, results can also be output in json format
% which facilitates storing in a SQL or NoSQL database like MongoDB. Setting up
% such a database solution is far beyond the scope of this demo, but the squall
% project (github.com/ikinsella/squall) is a developing solution that utilizes
% MongoDB to great effect.

##### SOURCE END #####
--></body></html>